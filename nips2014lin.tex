\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times,xcolor}
\usepackage[colorlinks, citecolor={green!70!black}]{hyperref}
\usepackage{url}
\usepackage{amsmath,amssymb}
\usepackage{booktabs,multirow}
\usepackage[backend=bibtex,
            style=numeric,
            firstinits=true,
            natbib=true,
            maxbibnames=10]{biblatex}
\usepackage{graphicx}
\usepackage{caption,subcaption}
\bibliography{nips2014lin}


\title{Extended and Unscented Gaussian Processes}


\author{
    Daniel M.~Steinberg, Edwin V.~Bonilla \\
    NICTA \\
    Sydney, Australia\\
    \texttt{\{daniel.steinberg,edwin.bonilla\}@nicta.com.au}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\input{notation}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}

    Two new methods for inference in Gaussian processes (GPs) with general
    nonlinear likelihoods are presented. Inference is based on a variational
    framework where a Gaussian posterior is assumed, then the likelihoods are
    linearized about the variational posterior mean using either a first order
    Taylor series expansion, or statistical linearization. We show that the
    parameter updates obtained by these algorithms are equivalent to the state
    update equations in the iterative extended and unscented Kalman filters
    respectively, hence we refer to our algorithms as extended and
    unscented GPs. The unscented GP treats the likelihood as a `black-box' by
    not requiring its derivative for inference, and can even use
    non-differentiable likelihood models. We demonstrate the algorithms on a
    number of toy inversion problems, and a binary classification dataset.

\end{abstract}

\section{Introduction}

This is how we will tell the story, and the points we need to address,
\begin{itemize}
    \item Something on the general problem and examples of it (inverse
        kinematics, navigation and state estimation).
    \item We are interested in the values of a latent process, where we have
        observations of this latent process that have been transformed through
        a known nonlinear function or `forward model'.
    \item We are also interesting in making predictions in both the latent
        process and observation space.
    \item Inference is intractable because we now have an arbitrary likelihood
        based on the form of the forward model.
    \item Like \cite{Opper2009} we use variational inference and assume a
        Gaussian posterior, but unlike \cite{Opper2009} we linearize the
        forward model to compute its expectation with respect the variational
        posterior.
    \item We show that using this variational framework we can derive the state
        update steps for the iterated extended \cite{Bell1993} and
        unscented \cite{Sibley2006} Kalman filters, depending on the 
        linearization method chosen.
    \item Ultimately we would like an algorithm that can use ``black-box''
        likelihood functions where the algorithm does need a derivative, or the
        likelihood may not even be differentiable.
\end{itemize}

Initially we formulate our model in \autoref{sec:gausmod} for the \emph{finite}
Gaussian case because the linearization methods are more generalised and
comparable with existing algorithms. Then in \autoref{sec:gpmod} we
specifically derive a factorising likelihood Gaussian process model using our
framework, which we use for experiments in \autoref{sec:experiments}.


\section{Nonlinear Gaussian Models}
\label{sec:gausmod}

Given some observable quantity $\obs \in \real{d}$, and a likelihood model for
the system we are interested in, $\probC{\obs}{\lstate}$, in many situations it
is desirable to know the latent input to the system, $\lstate\in\real{D}$, that
generated the observation. Finding these inputs is an inversion problem and in
a probabilistic setting can be cast as an application of Bayes' rule,
\begin{equation}
    \probC{\lstate}{\obs} = \frac{\probC{\obs}{\lstate}\prob{\lstate}}
        {\prob{\obs}}.
    \label{eq:bayesrule}
\end{equation}
The following forms are assumed for the prior and likelihood:
\begin{equation}
    \prob{\lstate} = \gausC{\lstate}{\prmean, \prcov}
    \quad \text{and} \quad
    \probC{\obs}{\lstate} = \gausC{\obs}{\nonlin{\lstate}, \lcov},
    \label{eq:priorlike}
\end{equation}
where $\nonlin{\cdot} : \real{D} \to \real{d}$ is a nonlinear function or
forward model. Unfortunately the marginal likelihood is intractable to compute
because the nonlinear function makes the likelihood and prior non-conjugate,
\begin{equation}
    \prob{\obs} = \int \gausC{\obs}{\nonlin{\lstate}, \lcov}
        \gausC{\lstate}{\prmean, \prcov} d\lstate,
    \label{eq:marglike}
\end{equation}
which also makes the posterior intractable to evaluate. Thus, we choose to
approximate the posterior using variational inference \cite{Jordan1999}.


\section{Variational Approximation}

Using variational inference, we can put a lower bound on the log-marginal
likelihood using Jensen's inequality, 
\begin{equation}
    \log \prob{\obs} \geq \int \qrob{\lstate} \log 
        \frac{\prob{\obs, \lstate}}{\qrob{\lstate}} d\lstate,
\end{equation}
with equality iff $\KL{\qrob{\lstate}}{\probC{\lstate}{\obs}} = 0$, and
$\qrob{\lstate}$ is an approximation to the true posterior,
\probC{\lstate}{\obs}. This lower bound is often referred to as `free energy',
and can be re-written as follows
\begin{equation}
    \Fengy = \expec{q\lstate}{\log \probC{\obs}{\lstate}}
        - \KL{\qrob{\lstate}}{\prob{\lstate}},
    \label{eq:fengy}
\end{equation}
where $\expec{q\lstate}{\cdot}$ is an expectation with respect to the
variational posterior, $\qrob{\lstate}$. We assume the posterior takes a
Gaussian form,
\begin{equation}
    \qrob{\lstate} = \gausC{\lstate}{\pomean, \pocov}. \label{eq:varpost}
\end{equation}
Now we can evaluate the expectation and KL term in \eqref{eq:fengy},
\begin{align}
    \expec{q\lstate}{\log \probC{\obs}{\lstate}}
        =& - \frac{1}{2} \sbrac{ 
            D\log{2\pi} + \log\deter{\lcov} 
            + \expec{q\lstate}{\brac{\obs - \nonlin{\lstate}}\transpose\lcov\inv
            \brac{\obs - \nonlin{\lstate}}}},
            \label{eq:qlike} \\
     - \KL{\qrob{\lstate}}{\prob{\lstate}}
        =& - \frac{1}{2} \sbrac{\log\deter{\prcov\pocov\inv}
            + \brac{\pomean - \prmean}\transpose\prcov\inv
            \brac{\pomean - \prmean}
            + \trace{\prcov\inv\pocov} - D } \label{eq:qkl}.
\end{align}
where the expectation involving $\nonlin{\cdot}$ may be intractable. One method
of dealing with these expectations is presented in \cite{Opper2009}; we provide
two alternatives based on methods of linearizing $\nonlin{\cdot}$.


\subsection{Learning the Parameters}

To find the optimal posterior mean, $\pomean$, we need to find the derivative,
\begin{equation}
    \frac{\partial\Fengy}{\partial\pomean} = -\frac{1}{2}
    \frac{\partial}{\pomean} \expec{q\lstate}{
        \brac{\lstate - \prmean}\transpose\prcov\inv
        \brac{\lstate - \prmean}
        + \brac{\obs - \nonlin{\lstate}}\transpose \lcov\inv
            \brac{\obs - \nonlin{\lstate}}},
\end{equation}
where all terms in $\Fengy$ independent of $\pomean$ have been dropped, and we
have placed the quadratic term from the KL term, \eqref{eq:qkl}, back into the
expectation. We can represent this as an augmented Gaussian,
\begin{equation}
    \frac{\partial\Fengy}{\partial\pomean} = -\frac{1}{2}
        \frac{\partial}{\pomean}
        \expec{q\lstate}{
        \brac{\augobs - \augnonlin{\lstate}}\transpose\augcov\inv
        \brac{\augobs - \augnonlin{\lstate}}
    },
    \label{eq:augsys}
\end{equation}
where
\begin{equation}
    \augobs = \begin{bmatrix} \obs \\ \mathbf{0} \end{bmatrix}, \quad
    \augnonlin{\lstate} = \begin{bmatrix} \nonlin{\lstate} \\ \lstate - \prmean 
        \end{bmatrix}, \quad
    \augcov = \begin{bmatrix} \lcov & \mathbf{0} \\ \mathbf{0} & \prcov 
        \end{bmatrix}.
\end{equation}
Now we can see solving for $\pomean$ is essentially a non-linear least squares 
problem\footnote{$-\frac{1}{2}
        \frac{\partial}{\partial\lstate}
        \brac{\augobs - \augnonlin{\lstate}}\transpose\augcov\inv
        \brac{\augobs - \augnonlin{\lstate}} = 0$.}, but about
the expected posterior value of $\lstate$. Even without the expectation, there
is no solution closed form solution to $\partial\Fengy/\partial\pomean = 0$.
To find a solution to this problem for the optimal $\pomean$ we can use the
Newton algorithm. It begins with an initial guess, $\pomean_0$, then proceeds
with the iterations,
\begin{equation}
    \pomean_{k+1} = \pomean_k -
    \step\brac{\nabla_\pomean\nabla_\pomean\Fengy}\inv \nabla_\pomean\Fengy,
    \label{eq:newton}
\end{equation}
for some step length, $\step \in (0, 1]$. Unfortunately, evaluating
$\nabla_\pomean\Fengy$ is still intractable because of the nonlinear term
within the expectation in \eqref{eq:augsys}. We can linearize
$\nonlin{\lstate}$, so we can then tractably evaluate the expectation,
\begin{equation}
    \nonlin{\lstate} \approx \Linmat\lstate + \intcpt,
    \label{eq:linearize}
\end{equation}
for some linearization matrix $\Linmat \in \real{d \times D}$ and an intercept
term $\intcpt \in \real{d}$. Using this approximation we get,
\begin{equation}
    \nabla_\pomean \Fengy
        \approx \Linmat\transpose\lcov\inv\brac{\obs - \Linmat\pomean 
            - \intcpt} - \prcov\inv\brac{\pomean - \prmean}
    \quad \text{and} \quad
    \nabla_\pomean \nabla_\pomean \Fengy
        \approx - \Linmat\transpose\lcov\inv\Linmat - \prcov\inv.
        \label{eq:deldelF}
\end{equation}
Substituting \eqref{eq:deldelF} into \eqref{eq:newton} and using the Woodbury
identity we get,
\begin{equation}
    \pomean_{k+1} = \brac{1-\step}\pomean_k + \step\prmean 
        + \step\Kgain_k\brac{\obs - \intcpt_k - \Linmat_k\prmean},
    \label{eq:pomean}
\end{equation}
where $\Kgain_k$ is a ``Kalman gain'' term,
\begin{equation}
    \Kgain_k = \prcov\Linmat_k\transpose\brac{\lcov +
        \Linmat_k\prcov\Linmat_k\transpose}\inv.
    \label{eq:kgain}
\end{equation}
and we have assumed that the linearization $\Linmat_k$ and intercept,
$\intcpt_k$ are in some way dependent on the iteration. We can find the 
posterior covariance by setting $\partial\Fengy/\partial\pocov = 0$ where,
\begin{equation}
    \frac{\partial\Fengy}{\partial\pocov} = -\frac{1}{2} 
        \frac{\partial}{\partial\pocov}
        \expec{q\lstate}{
            \brac{\augobs - \augnonlin{\lstate}}\transpose\augcov\inv
            \brac{\augobs - \augnonlin{\lstate}}
    } 
    + \frac{1}{2} \frac{\partial}{\partial\pocov} \log \deter{\pocov}.
\end{equation}
Again we do not have an analytic solution to this because we need to take
expectations and derivatives of terms involving $\nonlin{\lstate}$, so we use
the approximation in \eqref{eq:linearize} to get,
\begin{equation}
    \pocov = \sbrac{\prcov\inv + \Linmat\transpose\lcov\inv\Linmat}\inv
        = - \brac{\nabla_\pomean \nabla_\pomean \Fengy}\inv.
    \label{eq:pocovi}
\end{equation}
Using the Woodbury identity,
\begin{equation}
    \pocov = \brac{\ident{D} - \Kgain\Linmat}\!\prcov
    \label{eq:pocov}
\end{equation}
where the converged values of $\Linmat$ and $\Kgain$ are used.


\subsection{Taylor Series Linearization}

Now we need to find expressions for the linearization terms $\Linmat$ and
$\intcpt$. One method is to use a $1$st order Taylor Series Expansion to 
linearize $\nonlin{\cdot}$ about the last calculation of the posterior mean, 
$\pomean_k$,
\begin{equation}
    \nonlin{\lstate} \approx \nonlin{\pomean_k} +
    \jacob{\pomean_k}\brac{\lstate - \pomean_k},
\end{equation}
where $\jacob{\pomean_k}$ is the Jacobian $\partial\nonlin{\pomean_k}\!/ 
\partial\pomean_k$. Equating coefficients with \eqref{eq:linearize},
\begin{equation}
    \Linmat = \jacob{\pomean_k}, \qquad \intcpt = \nonlin{\pomean_k} -
    \jacob{\pomean_k}\pomean_k.
\end{equation}
Substituting these values into \eqref{eq:pomean} we get,
\begin{equation}
    \pomean_{k+1} = \brac{1-\step}\pomean_k + \step\prmean 
        + \step\Kgain_k\brac{\obs - \nonlin{\pomean_k} 
        + \jacob{\pomean_k}\brac{\pomean_k - \prmean}},
    \label{eq:pomean_ekf}
\end{equation}
where,
\begin{equation}
    \Kgain_k = \prcov\jacob{\pomean_k}\transpose\brac{\lcov +
        \jacob{\pomean_k}\prcov\jacob{\pomean_k}\transpose}\inv
    \quad\text{and}\quad
    \pocov = \brac{\ident{D} - \Kgain\jacob{\pomean}}\!\prcov.
    \label{eq:pocov_ekf}
\end{equation}
Here $\jacob{\pomean}$ and $\Kgain$ without the $k$ subscript are constructed
about the converged posterior, $\pomean$. It is worth noting that when $\step =
1$, \eqref{eq:pomean_ekf} -- \eqref{eq:pocov_ekf} are the \emph{same} as a
single step of the iterated extended Kalman filter~\cite{Bell1993, Sibley2006}.


\subsection{Statistical Linearization}

Another method for linearizing $\nonlin{\cdot}$ is \emph{statistical
    linearization} \cite{Geist2010}. Statistical linearization finds a least
squares best fit to $\nonlin{\cdot}$ about a point; in our case this point is
the \emph{posterior} mean, $\pomean$. The advantage of this method is that it
does not require derivatives $\partial\nonlin{\lstate}/\partial\lstate$. To
obtain the fit multiple observations of $\nonlin{\cdot}$ are required for
different input points. One method of obtaining these points is the unscented
transform \cite{Julier2004}, which defines $2D+1$ `sigma' points,
\begin{align}
    \Sfunc_0 &= \pomean,
        \label{eq:spoint0} \\
    \Sfunc_i &= \pomean + \brac{\sqrt{\brac{D + \scoef} \pocov}}_i \quad
        \text{for} \quad i = 1 \ldots D, \\
    \Sfunc_i &= \pomean - \brac{\sqrt{\brac{D + \scoef} \pocov}}_i \quad
        \text{for} \quad i = D+1 \ldots 2D,
        \label{eq:spointi} \\
    \Sobs_i &= \nonlin{\Sfunc_i}.
\end{align}
Here $(\sqrt{\cdot})_i$ refers to columns of the matrix square root, we follow
\cite{Julier2004} and use the Cholesky. Unlike the usual unscented transform,
which uses the prior to create the sigma points, here we have used the
posterior because of the expectation in \eqref{eq:augsys}. Using these points
we can define the following statistics,
\begin{equation}
    \bar\obs = \sum^{2D}_{i=0} \Sw_i \Sobs_i,
    \qquad
    \xcov = \sum^{2D}_{i=0} \Sw_i \brac{\Sobs_i - \bar\obs}
        \brac{\Sfunc_i - \pomean}\transpose,
    \label{eq:slstats}
\end{equation}
where,
\begin{equation}
    \Sw_0 = \frac{\scoef}{D + \scoef},
        \qquad \Sw_i = \frac{1}{2\brac{D + \scoef}}
        \quad \text{for} \quad i = 1 \ldots 2D,
    \label{eq:slweights}
\end{equation}
for a free parameter $\scoef$\footnote{Setting $\scoef = 0.5$ yields
    uniform weights.}. According to \cite{Julier2004} various settings of
$\scoef$ can capture information about the higher order moments of the
distribution of $\obs$. The objective of statistical linearization is,
\begin{equation}
    \argmin_{\Linmat, \intcpt} \sum^{2D}_{i=0} 
        \lnorm{2}{\Sobs_i - \brac{\Linmat\Sfunc_i + \intcpt}}^2.
\end{equation}
This is simply linear least-squares and has the solution \cite{Geist2010}:
\begin{equation}
    \Linmat = \xcov \pocov\inv, \qquad
    \intcpt = \bar\obs - \Linmat\pomean.
\end{equation}
Substituting $\intcpt$ back into \eqref{eq:pomean}, we obtain, 
\begin{equation}
    \pomean_{k+1} = \brac{1-\step}\pomean_k + \step\prmean 
        + \step\Kgain_k\brac{\obs - \bar\obs_k 
        + \Linmat_k\brac{\pomean_k - \prmean}}.
    \label{eq:pomean_sl}
\end{equation}
Here $\Kgain_k$, $\Linmat_k$ and $\bar\obs_k$ have been evaluated using the
statistics from the $k$th iteration. This implies that the posterior
covariance, $\pocov_k$, is now estimated at every iteration of
\eqref{eq:pomean_sl} since we use it to form $\Linmat_k$ and $\intcpt_k$, and 
$\Kgain_k$ and $\pocov_k$ have the same form as \eqref{eq:kgain} and
\eqref{eq:pocov} respectively. Equations \eqref{eq:pomean_sl} and
\eqref{eq:pocov} are similar to the equations for a single update of the
iterated sigma-point Kalman filter (iSPKF) \cite{Sibley2006} for $\step = 1$,
however we have the term $\bar\obs_k$ appearing in \eqref{eq:pomean_sl} as
opposed to $\nonlin{\pomean_k}$. This is more in-line with the regular
unscented Kalman filter \cite{Julier2004}, and statistically linearized
recursive least squares \cite{Geist2010}.


\subsection{Learning Objective Functions}

Because of the expectations involving an arbitrary function in
\eqref{eq:qlike}, no analytical solution exists for the lower bound on the
likelihood, $\Fengy$. However, we can use our approximation
\eqref{eq:linearize} to calculate $\Fengy$,
\begin{multline}
    \Fengy \approx - \frac{1}{2} \bigg[
        D\log{2\pi} + \log\deter{\lcov} + \log\deter{\prcov\pocov\inv}
    + \brac{\pomean - \prmean}\transpose\prcov\inv
        \brac{\pomean - \prmean} \\
    + \brac{\obs - \Linmat\pomean - \intcpt}\transpose\lcov\inv
        \brac{\obs - \Linmat\pomean - \intcpt}
        \bigg].
    \label{eq:approxfengy}
\end{multline}
Here the trace term from \eqref{eq:qkl} has cancelled with a trace term from
the expected likelihood, $\trace{\Linmat\transpose\lcov\inv\Linmat\pocov} = D -
\trace{\prcov\inv\pocov}$ once we have linearized $\nonlin{\cdot}$ and
substituted \eqref{eq:pocovi}. In practice we only calculate this approximation
$\Fengy$ if we need to optimise some model hyperparameters, like for a Gaussian
process. For optimising $\pomean$, the only dependent parts of $\Fengy$ on
$\pomean$ are,
\begin{equation}
    -\frac{1}{2} \brac{\obs - \nonlin{\pomean}}\transpose
            \lcov\inv
            \brac{\obs - \nonlin{\pomean}} \\
    -\frac{1}{2}
        \brac{\pomean - \prmean}\transpose
        \prcov\inv
        \brac{\pomean - \prmean},
    \label{eq:MAP}
\end{equation}
in the Taylor series linearization case. This is also the \emph{maximum
    a-posteriori} objective, and optimising this using Newton's method will
lead $\pomean$ to converge if $\pomean_0$ is sufficiently close to the true
posterior mean. No such guarantees exist for the statistical linearization
method, though \eqref{eq:MAP} also works well in practice, more discussion on
this point is in section \ref{sec:exptoy}. 


\section{Linearized Gaussian Processes}
\label{sec:gpmod}

We now present two predictive the Gaussian Process (GP) models
\cite{Rasmussen2006} with arbitrary non-linear likelihoods using the framework
presented previously. The Gaussian process has the following likelihood and
prior,
\begin{equation}
    \obs \sim \gaus{\nonlin{\lstate}, \lvar\ident{N}}, \qquad
    \lstate \sim \gaus{\mathbf{0}, \KERNL}.
    \label{eq:gplikeprior}
\end{equation}

Here $\obs \in \real{N}$ are the $N$ noisy observed values of the transformed
latent function, $\nonlin{\lstate}$, and $\lstate \in \real{N}$ is the latent
function we are interested in inferring. $\KERNL \in \real{N\times N}$ is the
kernel matrix, where each element $\kernl_{ij} = \kernl\!\brac{\inobs_i,
    \inobs_j}$ is the result of applying a kernel function to each input,
$\inobs \in \real{D}$, in a pair-wise manner. It is also important to note that
the likelihood noise model is \emph{isotropic} with a variance of $\lvar$. This
is not a necessary condition, and we can use a correlated noise likelihood
model, however the factorised likelihood case is still useful and provides some
computational benefits. 
    
As before, we make the approximation that the posterior is Gaussian,
$\qrobC{\lstate}{\pomean, \pocov} = \gausC{\lstate}{\pomean, \pocov}$ where
$\pomean \in \real{N}$ is the mean posterior latent function, and $\pocov \in
\real{N \times N}$ is the posterior covariance. Since the likelihood is
isotropic and factorises over $N$, we have the following expectation under
variational inference,
\begin{equation*}
    \expec{q\lstate}{\log\probC{\obs}{\lstate}} =
        - \frac{N}{2}\log{2\pi\lvar}
        - \frac{1}{2\lvar} \sum_{n=1}^N 
            \expec{q\lstates_n}{\brac{\obss_n - \nonlin{\lstates_n}}^2}.
\end{equation*}
As a consequence the linearization is scaler per element of $\lstate$,
\begin{equation}
    \nonlin{\lstates_n} \approx \Lins_n\lstates_n + \intcpts_n.
\end{equation}
Using this we can derive the approximate gradients,
\begin{equation}
    \nabla_\pomean\Fengy \approx \frac{1}{\lvar}\Linmat\brac{\obs -
        \Linmat\pomean - \intcpt} - \KERNL\inv\pomean,
    \qquad
    \nabla_\pomean\nabla_\pomean \Fengy \approx -\KERNL\inv
    -\Linmat\Lvar\inv\Linmat,
\end{equation}
where $\Linmat = \diag{\sbrac{\Lins_1, \ldots, \Lins_N}}$ and $\Lvar =
\diag{\sbrac{\lvar, \ldots, \lvar}}$. Because of the factorising likelihood we
obtain $\pocov\inv = \KERNL\inv + \Linmat\Lvar\inv\Linmat$, that is, the
inverse posterior covariance is just the prior inverse covariance, but with a
modified diagonal. This means if we were to use the natural parameterisation of
the Gaussian, we only have to infer $2N$ parameters (not $N + N^2$), which a
similar result to \cite{Opper2009}.  Straight-forwardly we can obtain the
Newton steps for the posterior mean,
\begin{equation}
    \pomean_{k+1} = \brac{1-\step}\pomean_k 
        + \step\Kgain_k\brac{\obs - \intcpt_k}.
\end{equation}
Also as before,
\begin{equation}
    \Kgain_k = \KERNL\Linmat_k\brac{\Lvar + \Linmat_k\KERNL\Linmat_k}\inv
    \quad \text{and} \quad
    \pocov = \brac{\ident{N} - \Kgain\Linmat}\!\KERNL.
\end{equation}
The values for $\Lins_n$ and $\intcpts_n$ for the Taylor series linearization
are now,
\begin{equation}
    \Lins_n = \frac{\partial\nonlin{\pomeans_n}}{\partial\pomeans_n},
    \qquad
    \intcpts_n = \nonlin{\pomeans_n}
        - \frac{\partial\nonlin{\pomeans_n}}{\partial\pomeans_n} \pomeans_n,
\end{equation}
and for statistical linearization,
\begin{equation}
    \Lins_n = \frac{\xcovs_{\pomeans\obss,n}}{\pocovs_{nn}},
    \qquad
    \intcpts_n = \bar\obss_n - \Lins_n\pomeans_n.
\end{equation}
$\pocovs_{nn}$ is the $n$th diagonal element of $\pocov$ and
$\xcovs_{\pomeans\obss,n}$ and $\bar\obss_n$ are calculated the same way as
\eqref{eq:spoint0} -- \eqref{eq:slweights} where the sigma points for each
observation $n \in N$ are $\Sfunc_{n} = \big\{\pomeans_n,~\pomeans_n +
\sqrt{\brac{1 + \scoef}\pocovs_{nn}},~\pomeans_n - \sqrt{\brac{1 +
        \scoef}\pocovs_{nn}}\big\}$ and the dimensionality is~1. We refer to
the Taylor series linearized GP as the \emph{extended} GP (EGP), and the
statistically linearized GP as the \emph{unscented} GP (UGP) to be consistent
with the literature on the EKF and UKF.

\subsection{Prediction}

To find the distribution for a test point, $\lstates\test$, we follow the
procedure in \cite{Rasmussen2006} for a Laplace GP classifier\footnote{More
    detail is provided in the supplementary material}. This gives
$\lstates\test \sim \gaus{\pomeans\test, \pocovs\test}$ where,
\begin{equation}
    \pomeans\test = \Kernl\testT\KERNL\inv\pomean,
    \qquad
    \pocovs\test = \kernl\ttest - \Kernl\testT\KERNL\inv\sbrac{\ident{N} -
        \pocov\KERNL\inv}\Kernl\test,
\end{equation}
and $\kernl\ttest = \kernl\!\brac{\inobs\test, \inobs\test}$ and $\Kernl\test
= \sbrac{\kernl\!\brac{\inobs_1, \inobs\test}, \ldots, \kernl\!\brac{\inobs_N,
        \inobs\test}}\transpose$. We can also find the predicted observations,
$\bar\obss\test$ by evaluating the one-dimensional integral,
\begin{equation}
    \bar\obss\test = \expec{q\lstates\test}{\obss\test} = \int
        \nonlin{\lstates\test} \gausC{\lstates\test}{\pomeans\test,
            \pocovs\test} d\lstates\test,
\end{equation}
for which we can use quadrature. Alternatively, if we were to use the UGP we
can use another application of the unscented transform to approximate the
predictive distribution $\obss\test \sim \gaus{\bar\obss\test,
    \lvar_{\obss\test}}$ and,
\begin{equation}
    \bar\obss\test = \sum^2_{i=0} \Sw_i \Sfunc_i\test, \qquad 
    \lvar_{\obss\test} = \sum^2_{i=0} \Sw_i \brac{\Sobs_i\test -
        \bar\obss\test}^2.
    \label{eq:slpred}
\end{equation}
which we find find works well in practice, see \autoref{fig:learnex} for a
demonstration.


\subsection{Learning}

Learning the extended and unscented GPs consists of in an inner and outer loop.
Much like the Laplace approximation for binary Gaussian Process classifiers
\cite{Rasmussen2006}, the inner loop is for learning the posterior mean,
$\pomean$, and the outer loop is to optimise the likelihood variance $\lvar$ 
and kernel hyperparameters $\khypers$.

A line search could be used to select an optimal value for the step length,
$\step$. However, we find that setting $\step=1$, and then successively
multiplying $\step$ by some number in $\brac{0, 1}$ until the MAP objective
\eqref{eq:MAP} lowers, or some maximum number of iterations is exceeded, works 
well in practice. If the maximum number of iterations is exceeded we call this
a `diverge' condition, and terminate the search of $\pomean$ (and return the
last good value). This only tends to happen for the unscented GP, and does not
tend to impact the algorithms performance, as demonstrated in 
\autoref{fig:learnex}.

\textbf{TODO: verify this --} The kernel hyperparameters,
$\kfunc{\cdot}{\cdot|\khypers}$, can be learned by optimising the gradients
$\partial\Fengy/\partial\khypers$. These gradients have both explicit and
implicit components ($\pomean$ is dependent on $\lvar$ and $\khypers$). Where
$\Fengy$ for the GPs is approximated as,
\begin{equation}
    \Fengy \approx - \frac{1}{2} \bigg[
        N\log{2\pi\lvar} + \log\deter{\KERNL\pocov\inv}
    + \pomean\transpose\KERNL\inv \pomean
    + \frac{1}{\lvar}
        \brac{\obs - \Linmat\pomean - \intcpt}\transpose\!
        \brac{\obs - \Linmat\pomean - \intcpt}
    \bigg].
    \label{eq:fengygp}
\end{equation}
Finding the implicit gradients for the unscented GP requires knowledge of
$\partial\nonlin{\lstates}\!/\partial\lstates$, which may not exist. We use
numerical techniques to find these gradients for both the extended and
unscented algorithms. In particular we use derivative-free optimisation methods
from the NLopt library \cite{JohnsonNLOPT}, which we find fast and highly
effective.


\section{Experiments}
\label{sec:experiments}

\subsection{Various Nonlinear Functions}
\label{sec:exptoy}

Experimental setup:
\begin{itemize}

    \item True latent function, $\lstate_{\text{true}}\!\brac{\inobs} =
        \sin\!\brac{\inobs} + \cos\!\brac{\pi\inobs}$.

    \item The observations are generated as $\obs =
        \nonlin{\lstate_\text{true}} + \epsilon$ where $\epsilon \sim \gaus{0,
            0.2^2}$.
    
    \item Matern $\frac{5}{2}$ covariance functions are used. All parameters
        are initialised at 1.0, and lower-bounded at 0.1 except for the noise,
        which is lower bounded at 0.01.

    \item Held out predictive negative log likelihood for the latent function
        is calculated as,
        \begin{equation}
            \frac{1}{2N\test} \sum^{N\test}_n \log{2\pi\pocovs\test_{nn}}
                + \frac{1}{\pocovs\test_{nn}} \brac{\lstates\test_n -
                    \pomeans\test_n}^2.
        \end{equation}

    \item 5-fold cross validation, 400 points training, 100 points testing.

\end{itemize}

\begin{figure}[htb]
    \begin{subfigure}[b]{0.5\linewidth}
        \includegraphics[width=\linewidth]{fig/signdemo}
        \caption{$\nonlin{\lstate} = 2\times\text{sign}\!\brac{\lstate}
            + \lstate^3$}
        \label{sub:sign}
    \end{subfigure}
    %\begin{subfigure}[b]{0.5\linewidth}
        %\includegraphics[width=\linewidth]{fig/rounddemo}
        %\caption{$\nonlin{\lstate} = \text{round}\!\brac{5\lstate}$}
        %\label{sub:rnd}
    %\end{subfigure} \\
    %\begin{subfigure}[b]{0.5\linewidth}
        %\includegraphics[width=\linewidth]{fig/trace_beg}
        %\caption{MAP trace -- poor hyperparameter values}
        %\label{sub:mapb}
    %\end{subfigure}
    \begin{subfigure}[b]{0.5\linewidth}
        \includegraphics[width=\linewidth]{fig/trace_end}
        \caption{MAP trace -- near optimal hyperprameter values}
        \label{sub:mape}
        \vspace{1.5mm}
    \end{subfigure}

    \caption[]{Example of learning the UGP with a non-differentiable nonlinear
        function in (\subref{sub:sign}). In this figure we show the predicitve
        distribution of $\expec{q\lstates}{\obss\test}$ using
        \eqref{eq:slpred}. A typical trace from the MAP objective function used
        to learn $\pomean$ from the non-linear function in (\subref{sub:sign})
        is shown in (\subref{sub:mape}. The optimisation shown terminated
        because of a `divergence' condition, which was typical for this
        example.}

    \label{fig:learnex}
\end{figure}

\begin{table}[htb]
    \centering
    \small
    \caption[]{Results from running nonlinear likelihood GPs with various
        differentiable nonlinear functions. The true latent function is
        $\lstate_{\text{true}}\!\brac{\inobs} = \sin\!\brac{\inobs} 
            + \cos\!\brac{\pi\inobs}$.}
    \begin{tabular}{r|c| c c c c c c}
        \multirow{2}{*}{$\nonlin{\lstate}$} & \multirow{2}{*}{Algorithm} & 
            \multicolumn{2}{c}{$-\frac{1}{N\test}
                \log\probC{\lstates\test}{\obs}$} &
            \multicolumn{2}{c}{SMSE $\lstates\test$ (\expon{}{-4})} &
            \multicolumn{2}{c}{SMSE $\obss\test$ (\expon{}{-4})} \\
        & & mean & std. & mean & std. & mean & std.\\
        \toprule
        $\lstate$ 
            & UGP & $-$1.4437 & 0.0472 & 37.476 & 4.777 & -- & -- \\
            & EGP & $-$1.4840 & 0.0984 & 35.252 & 8.348 & -- & -- \\
            & \cite{Opper2009} \\
            & GP & $-$1.2970 & 0.0487 & 34.728 & 10.264 & -- & -- \\
        \midrule
        $\lstate^3 + \lstate^2 + \lstate$ 
            & UGP & $-$2.1587 & 0.0681 & 26.946 & 5.936 & 36.043 & 4.971 \\
            & EGP & $-$2.1595 & 0.0524 & 26.520 & 5.538 & 36.115 & 5.078 \\
            & \cite{Opper2009} \\
        \midrule
        $\exp\!\brac{\lstate}$ 
            & UGP & $-$1.2993 & 0.2028 & 129.22 & 76.450 & 173.28 & 27.692 \\
            & EGP & $-$1.3136 & 0.1878 & 112.25 & 63.446 & 173.30 & 27.918 \\
            & \cite{Opper2009} \\
        \midrule
        $\sin\!\brac{\lstate}$ 
            & UGP & $-$0.8904 & 0.1214 & 151.16 & 30.212 & 858.74 & 104.04 \\
            & EGP & $-$0.8170 & 0.1552 & 173.06 & 41.686 & 858.33 & 103.47 \\
            & \cite{Opper2009} \\
        \midrule
        $\tanh\!\brac{2\lstate}$
            & UGP & $-$0.6060 & 0.1240 & 354.57 & 66.352 & 604.98 & 62.572 \\
            & EGP & $-$0.4592 & 0.1805 & 476.43 & 108.81 & 610.52 & 62.544 \\
            & \cite{Opper2009} \\
        \bottomrule
    \end{tabular}
\end{table}


\textbf{TODO:} Discuss divergence behaviour of statistical linearization.


\subsection{Binary Handwritten Digit Classification}

We use the error rate (percent incorrectly predicted labels) and average 
negative log Bernoulli likelihood to asses performance,
\begin{equation}
    - \frac{1}{N\test} \sum^{N\test}_{n} \obss\test_n 
        \log\brac{\pi\test_n}
    + \brac{1-\obss\test_n}\log\brac{1 - \pi\test_n},
\end{equation}
where $\pi\test_n = \expec{}{\prob{\obss\test_n = 1}}$ is predicted from the
classifications algorithms.

A square exponential kernel with amplitude $\sigma_\text{se}$ and length
scale $l_\text{se}$ is used for the Gaussian processes in this experiment.

Other experimental settings,
\begin{itemize}
    \item Initial hyperparameters $\sigma_{\text{se},0} = l_{\text{se},0} = 1$, 
        lower bounds are $0.1$. $\lstd_0 = 10^{-6}$, lower bound $10^{-7}$.
\end{itemize}

\begin{table}[htb]
    \centering
    \small
    \caption[]{Binary Gaussian process classifier performance on the USPS
        handwritten-digits dataset for numbers `3' and `5'.}
    \begin{tabular}{r| c c c c}
        Algorithm & Av.\ neg.\ ll. & Error rate (\%) 
            & $\log\!\brac{\sigma_\text{se}}$ & $\log\!\brac{l_\text{se}}$ \\
        \toprule
        GP -- Laplace & 0.11528 & 2.9754 & 2.5855 & 2.5823 \\
        GP -- EP & 0.07522 & 2.4580 & 5.2209 & 2.5315 \\
        SVM (RBF) & 0.08055 & 2.3286 & -- & -- \\
        Logistic Reg. & 0.11995 & 3.6223 & -- & -- \\
        \midrule
        UGP & 0.06524 & 2.1992 & 1.5140 & 1.4257 \\
        EGP & 0.12946 & 2.0699 & 3.3068 & 1.7480 \\
        %UGP & 0.11748 & 2.9754 &  2.8177 & 2.5504 \\
        %EGP & 0.69293 & 42.4321 & 3.1268 & $-$0.1696 \\
        %EGP & 0.69315 & 54.0750 & 1.1387 & $-$2.3530 \\
        %EGP* & 0.1189 & 2.1992 & 3.4495 & 1.8866 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Conclusion}

Future work:
\begin{itemize}
    \item Use a fully correlated noise model E/UGP.
    \item Extend this to a multiple output/task E/UGP so it can be used for a
        wide range of inversion problems, such as inverse kinematics.
    \item Hyperparameter gradients.
\end{itemize}

\subsubsection*{Acknowledgments}

\subsubsection*{References}
\printbibliography

\end{document}
